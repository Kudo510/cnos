{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Patch descriptors register in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 load and pachify depth and rgb in 14*14 size patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "\n",
    "scene_id = 1\n",
    "frame_id = 0\n",
    "## 10* use :06d to turn to 6 digit - but also 10* to turn back just use int as str(int(scene_id)) \n",
    "scene_id = f\"{scene_id:06d}\"\n",
    "frame_id = f\"{frame_id:06d}\"\n",
    "\n",
    "\n",
    "\n",
    "rgb_img = np.array(Image.open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/rgb/{frame_id}.png\"))\n",
    "# depth image with pixel values as float (16 bit)\n",
    "depth_img = np.array(Image.open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/depth/{frame_id}.png\"), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "## To divide image into patches using patchify(image, patch_shape, step) - input as numpy image of (W,H,3)\n",
    "## step in in pat defines the distance between one patch and the next one (vertically and horizontally). If step ≥ patch_height there is no overlap between patches in the same row. If step ≥ patch_width there is no overlap between patches in the same column.\n",
    "## step=patch_size to make sure there is no overlap\n",
    "# First, ensure the image dimensions are divisible by the patch size\n",
    "# Ensure the image dimensions are divisible by the patch size\n",
    "def pad_to_multiple(array, multiple):\n",
    "    height, width, channels = array.shape\n",
    "    pad_height = (multiple - height % multiple) % multiple\n",
    "    pad_width = (multiple - width % multiple) % multiple\n",
    "    padding = [(0, pad_height), (0, pad_width), (0, 0)]  # padd to the b\n",
    "    return np.pad(array, padding, mode='constant')\n",
    "\n",
    "#? Pad 0 to depth image might be wrong- should be infinite instead\n",
    "patch_size = 14 \n",
    "padded_rgb_img = pad_to_multiple(rgb_img, patch_size) #(490, 644, 3)\n",
    "padded_depth_img = pad_to_multiple(np.expand_dims(depth_img, axis=-1), patch_size).squeeze() #(490, 644)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify\n",
    "rgb_patches = patchify(padded_rgb_img, (14,14,3), step=patch_size) # patch shape [14,14,3]\n",
    "depth_patches = patchify(padded_depth_img, (14,14), step=patch_size) # patch shape [14,14]\n",
    "# ## Merge the patches to 1 image again using unpatchify\n",
    "# reconstructed_image = unpatchify(patches, image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 46, 14, 14)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_patches.shape # (35, 46, 1, 14, 14, 3) ## means that we have create 35*46*1 = 1610 patches - 10* 1 is for the channel here only 1 patch for the channel\n",
    "depth_patches.shape # (35, 46, 14, 14) ## means that we have create 35*46 = 1610 patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Filter invalid patches\n",
    "Use the mask images to filter out these patches - if its center doesn't belong to the mask then is excluded\n",
    "so we divide the rbg and masks in the same way then can just check the validity within the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the masks - we have mutiple visible masks for the input iamge- sum it up (clip it in range of 0,1)- then u have the mask for the entire scene\n",
    "# count the number of objects in the scene/frame via scene_gt_info.json\n",
    "\n",
    "import json\n",
    "scene_gt = json.load(open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/scene_gt_info.json\", \"r\"))\n",
    "num_objs = len(scene_gt[str(int(frame_id))])\n",
    "\n",
    "masks = list()\n",
    "for obj_id in range(num_objs):\n",
    "    mask = np.array(Image.open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/mask_visib/{frame_id}_{obj_id:06d}.png\"))\n",
    "    masks.append(mask)\n",
    "\n",
    "entire_mask = np.clip(np.sum(np.array(masks), axis=0), 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 46, 14, 14)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad and patchify the mask\n",
    "padded_mask = pad_to_multiple(np.expand_dims(entire_mask, axis=-1), patch_size).squeeze()\n",
    "mask_patches = patchify(padded_mask, (14,14), step=patch_size) # patch shape [14,14]\n",
    "mask_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallen the patches for easier iteration\n",
    "rgb_patches = rgb_patches.reshape(-1, patch_size, patch_size, 3)\n",
    "mask_patches = mask_patches.reshape(-1, patch_size, patch_size)\n",
    "depth_patches = depth_patches.reshape(-1, patch_size, patch_size)\n",
    "\n",
    "valid_rgb_patches = []\n",
    "valid_mask_patches = []\n",
    "valid_depth_patches = []\n",
    "\n",
    "## Center of an image is height/2, width/2 and here is (7,7)\n",
    "for rgb_patch, mask_patch, depth_patch in zip(rgb_patches, mask_patches, depth_patches):\n",
    "    center_y, center_x = patch_size // 2, patch_size // 2\n",
    "    if mask_patch[center_y, center_x] == 1:  # Check if the center belongs to the foreground\n",
    "        valid_rgb_patches.append(rgb_patch)\n",
    "        valid_mask_patches.append(mask_patch)\n",
    "        valid_depth_patches.append(depth_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307237/1390511641.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  valid_rgb_patches = torch.tensor(valid_rgb_patches).permute(0,3,1,2)\n"
     ]
    }
   ],
   "source": [
    "# Permute to tensor with shaape (num_patches, 3, 14, 14)\n",
    "valid_rgb_patches = torch.tensor(valid_rgb_patches).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalid_mask_patches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_depth_patches = torch.tensor(np.array(valid_depth_patches))\n",
    "valid_mask_patches = torch.tensor(np.array(valid_mask_patches, dtype=\"uint8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load features via Dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307237/531409554.py:7: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"configs\")\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# Initialize Hydra and compose the configuration\n",
    "initialize(config_path=\"configs\")\n",
    "cfg = compose(config_name=\"run_inference\")\n",
    "OmegaConf.set_struct(cfg, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "# define the cnos model\n",
    "cfg_segmentor = cfg.model.segmentor_model\n",
    "if \"fast_sam\" in cfg_segmentor._target_:\n",
    "    logging.info(\"Using FastSAM, ignore stability_score_thresh!\")\n",
    "else:\n",
    "    cfg.model.segmentor_model.stability_score_thresh = 0.97 # for sam\n",
    "cfg.model.descriptor_model.model.num_block = 19\n",
    "\n",
    "model = instantiate(cfg.model).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.descriptor_model.model.num_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def patches_feature_extraction(valid_rgb_patches, cnos_model):\n",
    "    # crop_rgb: numpy array\n",
    "    temps = np.transpose(np.array(valid_rgb_patches), (0,2,3,1))\n",
    "    rgb_normalize = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "    normalized_patches = torch.stack([rgb_normalize(patch) for patch in temps])\n",
    "    with torch.no_grad(): \n",
    "        feature_patches= cnos_model.descriptor_model.model(normalized_patches.to(\"cuda\"))\n",
    "    return feature_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_patch_features = patches_feature_extraction(valid_rgb_patches, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([309, 1024])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_patch_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bag-of-words descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
