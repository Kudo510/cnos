{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Patch descriptors register in 3D for an rgb image - not what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 load and pachify depth and rgb in 14*14 size patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "\n",
    "scene_id = 1\n",
    "frame_id = 0\n",
    "## 10* use :06d to turn to 6 digit - but also 10* to turn back just use int as str(int(scene_id)) \n",
    "scene_id = f\"{scene_id:06d}\"\n",
    "frame_id = f\"{frame_id:06d}\"\n",
    "\n",
    "\n",
    "\n",
    "rgb_img = np.array(Image.open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/rgb/{frame_id}.png\"))\n",
    "# depth image with pixel values as float (16 bit)\n",
    "depth_img = np.array(Image.open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/depth/{frame_id}.png\"), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "## To divide image into patches using patchify(image, patch_shape, step) - input as numpy image of (W,H,3)\n",
    "## step in in pat defines the distance between one patch and the next one (vertically and horizontally). If step ≥ patch_height there is no overlap between patches in the same row. If step ≥ patch_width there is no overlap between patches in the same column.\n",
    "## step=patch_size to make sure there is no overlap\n",
    "# First, ensure the image dimensions are divisible by the patch size\n",
    "# Ensure the image dimensions are divisible by the patch size\n",
    "def pad_to_multiple(array, multiple):\n",
    "    height, width, channels = array.shape\n",
    "    pad_height = (multiple - height % multiple) % multiple\n",
    "    pad_width = (multiple - width % multiple) % multiple\n",
    "    padding = [(0, pad_height), (0, pad_width), (0, 0)]  # padd to the b\n",
    "    return np.pad(array, padding, mode='constant')\n",
    "\n",
    "#? Pad 0 to depth image might be wrong- should be infinite instead\n",
    "patch_size = 14 \n",
    "padded_rgb_img = pad_to_multiple(rgb_img, patch_size) #(490, 644, 3)\n",
    "padded_depth_img = pad_to_multiple(np.expand_dims(depth_img, axis=-1), patch_size).squeeze() #(490, 644)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify\n",
    "rgb_patches = patchify(padded_rgb_img, (14,14,3), step=patch_size) # patch shape [14,14,3]\n",
    "depth_patches = patchify(padded_depth_img, (14,14), step=patch_size) # patch shape [14,14]\n",
    "# ## Merge the patches to 1 image again using unpatchify\n",
    "# reconstructed_image = unpatchify(patches, image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 46, 14, 14)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_patches.shape # (35, 46, 1, 14, 14, 3) ## means that we have create 35*46*1 = 1610 patches - 10* 1 is for the channel here only 1 patch for the channel\n",
    "depth_patches.shape # (35, 46, 14, 14) ## means that we have create 35*46 = 1610 patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Filter invalid patches\n",
    "Use the mask images to filter out these patches - if its center doesn't belong to the mask then is excluded\n",
    "so we divide the rbg and masks in the same way then can just check the validity within the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the masks - we have mutiple visible masks for the input iamge- sum it up (clip it in range of 0,1)- then u have the mask for the entire scene\n",
    "# count the number of objects in the scene/frame via scene_gt_info.json\n",
    "\n",
    "import json\n",
    "scene_gt = json.load(open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/scene_gt_info.json\", \"r\"))\n",
    "num_objs = len(scene_gt[str(int(frame_id))])\n",
    "\n",
    "masks = list()\n",
    "for obj_id in range(num_objs):\n",
    "    mask = np.array(Image.open(f\"datasets/bop23_challenge/datasets/icbin/test/{scene_id}/mask_visib/{frame_id}_{obj_id:06d}.png\"))\n",
    "    masks.append(mask)\n",
    "\n",
    "entire_mask = np.clip(np.sum(np.array(masks), axis=0), 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 46, 14, 14)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad and patchify the mask\n",
    "padded_mask = pad_to_multiple(np.expand_dims(entire_mask, axis=-1), patch_size).squeeze()\n",
    "mask_patches = patchify(padded_mask, (14,14), step=patch_size) # patch shape [14,14]\n",
    "mask_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallen the patches for easier iteration\n",
    "rgb_patches = rgb_patches.reshape(-1, patch_size, patch_size, 3)\n",
    "mask_patches = mask_patches.reshape(-1, patch_size, patch_size)\n",
    "depth_patches = depth_patches.reshape(-1, patch_size, patch_size)\n",
    "\n",
    "valid_rgb_patches = []\n",
    "valid_mask_patches = []\n",
    "valid_depth_patches = []\n",
    "\n",
    "## Center of an image is height/2, width/2 and here is (7,7)\n",
    "for rgb_patch, mask_patch, depth_patch in zip(rgb_patches, mask_patches, depth_patches):\n",
    "    center_y, center_x = patch_size // 2, patch_size // 2\n",
    "    if mask_patch[center_y, center_x] == 1:  # Check if the center belongs to the foreground\n",
    "        valid_rgb_patches.append(rgb_patch)\n",
    "        valid_mask_patches.append(mask_patch)\n",
    "        valid_depth_patches.append(depth_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307237/1390511641.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  valid_rgb_patches = torch.tensor(valid_rgb_patches).permute(0,3,1,2)\n"
     ]
    }
   ],
   "source": [
    "# Permute to tensor with shaape (num_patches, 3, 14, 14)\n",
    "valid_rgb_patches = torch.tensor(valid_rgb_patches).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalid_mask_patches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_depth_patches = torch.tensor(np.array(valid_depth_patches))\n",
    "valid_mask_patches = torch.tensor(np.array(valid_mask_patches, dtype=\"uint8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load features via Dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307237/531409554.py:7: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"configs\")\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# Initialize Hydra and compose the configuration\n",
    "initialize(config_path=\"configs\")\n",
    "cfg = compose(config_name=\"run_inference\")\n",
    "OmegaConf.set_struct(cfg, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "# define the cnos model\n",
    "cfg_segmentor = cfg.model.segmentor_model\n",
    "if \"fast_sam\" in cfg_segmentor._target_:\n",
    "    logging.info(\"Using FastSAM, ignore stability_score_thresh!\")\n",
    "else:\n",
    "    cfg.model.segmentor_model.stability_score_thresh = 0.97 # for sam\n",
    "cfg.model.descriptor_model.model.num_block = 19\n",
    "\n",
    "model = instantiate(cfg.model).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.descriptor_model.model.num_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def patches_feature_extraction(valid_rgb_patches, cnos_model):\n",
    "    # crop_rgb: numpy array\n",
    "    temps = np.transpose(np.array(valid_rgb_patches), (0,2,3,1))\n",
    "    rgb_normalize = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "    normalized_patches = torch.stack([rgb_normalize(patch) for patch in temps])\n",
    "    with torch.no_grad(): \n",
    "        feature_patches= cnos_model.descriptor_model.model(normalized_patches.to(\"cuda\"))\n",
    "    return feature_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_patch_features = patches_feature_extraction(valid_rgb_patches, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([309, 1024])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_patch_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Patch descriptors register in 3D for 642 templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load templates and divide into patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "# Load templates\n",
    "template_path = \"datasets/bop23_challenge/datasets/templates_pyrender/icbin_642/obj_000001\"\n",
    "template_files = sorted(glob.glob(os.path.join(template_path, \"*.png\")), key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "templates = [np.array(Image.open(template_file).convert(\"RGB\").resize((420,420)))[:,:,:3] for template_file in template_files] # This image has 4 channels- the last one is not crucial - maybe about opacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify\n",
    "# Divide each template into patches size 14*14*3\n",
    "patch_size = 14\n",
    "templates_patches = [patchify(template, (14,14,3), step=patch_size) for template in templates] \n",
    "# Shape(30, 30, 1, 14, 14, 3) for each patch - Flaten this array \n",
    "templates_patches = np.array([templates_patch.reshape(-1,14,14,3) for templates_patch in templates_patches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 900, 14, 14, 3)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filter out invalid patches\n",
    "10* mask out the templates - clamp/clip at [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out the templates by clampping at 0,1 \n",
    "\n",
    "masks_patches = np.clip(np.sum(templates_patches, axis=-1), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Center of an image is height/2, width/2 and here is (7,7)\n",
    "valid_templates_patches = list()\n",
    "valid_masks_patches = list()\n",
    "center_y, center_x = patch_size // 2, patch_size // 2\n",
    "\n",
    "for template_patches, mask_patches in zip(templates_patches, masks_patches):\n",
    "    valid_template_patches = list()\n",
    "    valid_mask_patches = list()\n",
    "    for template_patch, mask_patch in zip(template_patches, mask_patches):\n",
    "        if mask_patch[center_y, center_x] == 1:  # Check if the center belongs to the foreground\n",
    "            valid_template_patches.append(template_patch)\n",
    "            valid_mask_patches.append(mask_patch)\n",
    "    valid_templates_patches.append(np.array(valid_template_patches))\n",
    "    valid_masks_patches.append(np.array(valid_mask_patches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 14, 14)\n",
      "(33, 14, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "# Each template has different number of valid templates\n",
    "print(valid_masks_patches[0].shape)\n",
    "print(valid_templates_patches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642 [33, 35, 35, 36, 36, 34, 39, 41, 39, 37, 39, 41, 40, 41, 44, 41, 45, 44, 45, 45, 44, 43, 46, 45, 42, 42, 46, 47, 46, 46, 44, 50, 49, 46, 46, 48, 49, 50, 48, 50, 49, 47, 49, 49, 45, 45, 51, 49, 51, 53, 49, 53, 52, 51, 53, 50, 50, 53, 53, 50, 50, 52, 53, 53, 52, 54, 53, 54, 54, 50, 50, 55, 53, 55, 54, 51, 56, 55, 54, 52, 55, 55, 56, 56, 56, 54, 52, 55, 55, 54, 53, 54, 55, 56, 56, 56, 57, 56, 55, 54, 54, 55, 57, 58, 58, 55, 58, 57, 58, 57, 56, 56, 58, 58, 56, 56, 59, 58, 58, 59, 57, 56, 58, 59, 56, 56, 58, 59, 57, 58, 58, 59, 58, 59, 55, 56, 58, 59, 59, 59, 56, 60, 59, 57, 57, 59, 60, 58, 60, 60, 57, 57, 59, 59, 57, 57, 58, 60, 60, 60, 57, 57, 60, 60, 55, 57, 58, 59, 56, 58, 60, 60, 60, 60, 55, 56, 56, 59, 60, 59, 55, 60, 57, 60, 57, 56, 56, 57, 56, 59, 60, 59, 57, 60, 59, 57, 56, 59, 59, 56, 56, 57, 58, 57, 56, 60, 59, 59, 59, 56, 56, 58, 58, 57, 58, 59, 59, 60, 58, 56, 56, 60, 58, 57, 58, 59, 60, 58, 60, 58, 56, 57, 57, 58, 59, 59, 58, 59, 60, 58, 57, 58, 59, 60, 56, 57, 58, 58, 60, 59, 56, 58, 59, 57, 59, 60, 58, 60, 59, 56, 56, 60, 58, 60, 58, 57, 57, 58, 58, 58, 59, 60, 59, 60, 58, 56, 58, 58, 58, 59, 58, 58, 59, 58, 59, 61, 59, 59, 59, 56, 56, 58, 58, 60, 58, 58, 59, 59, 59, 56, 57, 58, 58, 58, 58, 58, 58, 56, 56, 56, 56, 56, 58, 58, 58, 59, 57, 59, 58, 59, 60, 61, 61, 61, 61, 60, 58, 57, 57, 57, 57, 57, 56, 56, 57, 56, 57, 56, 56, 57, 58, 55, 54, 53, 53, 55, 54, 54, 55, 55, 54, 54, 55, 53, 54, 54, 54, 57, 54, 56, 53, 53, 53, 56, 55, 55, 54, 55, 54, 54, 55, 53, 53, 55, 55, 55, 54, 54, 55, 55, 55, 59, 57, 55, 53, 58, 54, 55, 58, 57, 57, 57, 56, 53, 56, 58, 57, 58, 56, 54, 58, 57, 58, 54, 57, 58, 56, 56, 58, 58, 58, 58, 59, 56, 58, 58, 56, 58, 58, 59, 56, 59, 59, 58, 58, 58, 57, 59, 57, 59, 57, 59, 58, 57, 58, 59, 57, 58, 60, 58, 59, 58, 58, 57, 57, 59, 57, 59, 58, 59, 58, 57, 58, 59, 57, 60, 57, 58, 59, 58, 59, 59, 57, 58, 60, 57, 59, 59, 56, 57, 56, 57, 60, 57, 60, 58, 59, 57, 56, 57, 60, 57, 56, 58, 57, 59, 59, 57, 56, 56, 60, 58, 56, 58, 58, 56, 56, 59, 58, 56, 58, 56, 56, 57, 59, 57, 56, 56, 56, 56, 57, 56, 59, 57, 57, 58, 56, 56, 56, 56, 58, 55, 57, 57, 57, 55, 56, 55, 56, 56, 56, 55, 57, 57, 55, 57, 55, 56, 56, 56, 55, 55, 54, 55, 57, 56, 58, 56, 54, 55, 55, 54, 54, 54, 54, 56, 55, 55, 56, 55, 55, 54, 55, 55, 55, 55, 55, 54, 54, 55, 55, 55, 55, 53, 53, 53, 53, 55, 53, 55, 54, 54, 55, 52, 52, 54, 54, 54, 55, 54, 55, 53, 53, 53, 53, 51, 52, 51, 49, 51, 53, 51, 50, 51, 54, 51, 50, 52, 50, 50, 54, 49, 48, 50, 52, 50, 50, 48, 48, 47, 49, 48, 48, 48, 48, 49, 47, 48, 48, 48, 46, 47, 47, 49, 48, 47, 49, 50, 48, 49, 48, 47]\n"
     ]
    }
   ],
   "source": [
    "# Number of valid patches on each template\n",
    "num_valid_patches = [valid_template_patches.shape[0] for valid_template_patches in valid_templates_patches] \n",
    "print(len(num_valid_patches), num_valid_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract patch descriptors using Dinvo ViT L14 with DINOv2 with registers\n",
    "Flatten the valid patches into shape (num_valid_patches, 14,14,3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# Initialize Hydra and compose the configuration\n",
    "# initialize(config_path=\"configs\")\n",
    "cfg = compose(config_name=\"run_inference\")\n",
    "OmegaConf.set_struct(cfg, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "# choose only 18 first layers\n",
    "cfg.model.descriptor_model.model.num_block = 18\n",
    "model = instantiate(cfg.model).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def patches_feature_extraction(valid_template_patches, cnos_model):\n",
    "    # crop_rgb: numpy array\n",
    "    # temps = np.transpose(np.array(valid_template_patches), (0,2,3,1))\n",
    "    rgb_normalize = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "    normalized_patches = torch.stack([rgb_normalize(patch) for patch in valid_template_patches])\n",
    "    with torch.no_grad(): \n",
    "        feature_patches= cnos_model.descriptor_model.model(normalized_patches.to(\"cuda\"))\n",
    "    return feature_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the valid patches into shape (num_valid_patches, 14,14,3 )\n",
    "valid_templates_patches = np.concatenate(valid_templates_patches, axis=0)\n",
    "valid_masks_patches = np.concatenate(valid_masks_patches, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35436, 14, 14)\n",
      "(35436, 14, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "print(valid_masks_patches.shape)\n",
    "print(valid_templates_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35436, 1024])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates_patches_features = patches_feature_extraction(valid_templates_patches, model)\n",
    "templates_patches_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35436, 256)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with the 256 components\n",
    "pca = PCA(n_components=256)\n",
    "pca_patches_descriptors = pca.fit_transform(np.array(templates_patches_features.cpu()))\n",
    "pca_patches_descriptors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Perform Kmean clustering for all patch descriptors from templates (2048 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finding visual words as centroids of the descriptors\n",
    "# import torch\n",
    "# from torch_kmeans import KMeans\n",
    "\n",
    "# model = KMeans(n_clusters=256)\n",
    "\n",
    "# # Input size is (BS, N, D) \n",
    "# # only testing with the first \n",
    "\n",
    "# result = model(torch.tensor(pca_patches_descriptors).cuda().unsqueeze(0))\n",
    "# print(result.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MiniBatchKMeans(n_clusters=2048)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MiniBatchKMeans<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.MiniBatchKMeans.html\">?<span>Documentation for MiniBatchKMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MiniBatchKMeans(n_clusters=2048)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MiniBatchKMeans(n_clusters=2048)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "total_clusters = 2048\n",
    "# Initialize the K-Means model\n",
    "kmeans = MiniBatchKMeans(n_clusters = total_clusters)\n",
    "# Fitting the model to training set\n",
    "kmeans.fit(pca_patches_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 901,   28,   23, ..., 1828, 1463, 1177], dtype=int32)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_labels = list()\n",
    "start_idx = 0\n",
    "for num in num_valid_patches:\n",
    "    end_idx = start_idx + num\n",
    "    template_labels = kmeans.labels_[start_idx:end_idx]\n",
    "    templates_labels.append(template_labels)\n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307237/601609967.py:17: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  bi = n_it / nt * math.log(N / ni)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_templates_vector(templates_labels, num_clusters = 2048):\n",
    "    # Calculate bag-of-words descriptors of the templates\n",
    "\n",
    "    templates_vector = list()\n",
    "    all_occurrences = [np.bincount(templates_label, minlength=2048) for templates_label in templates_labels]\n",
    "    ni_array = np.sum(np.array(all_occurrences), axis = 0)\n",
    "    N = len(templates_labels) # Number of templates\n",
    "    for t in range(len(templates_labels)):\n",
    "        template_vector = list()\n",
    "        occurrences = np.bincount(templates_labels[t], minlength=2048)\n",
    "        for i in range(num_clusters):\n",
    "            n_it = occurrences[i] + 1\n",
    "            nt = len(templates_labels[t]) + 1\n",
    "            ni = ni_array[i]\n",
    "            bi = n_it / nt * math.log(N / ni)\n",
    "            template_vector.append(bi)\n",
    "        templates_vector.append(np.array(template_vector))\n",
    "    return templates_vector\n",
    "templates_vector = calculate_templates_vector(templates_labels = templates_labels, num_clusters = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14936159, 0.1035338 , 0.10512402, ..., 0.14936159, 0.12241186,\n",
       "       0.11960862])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates_vector[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieving similar templates\n",
    "Warp/crop object of image query\n",
    "\n",
    "Resize crop to 420,420,3\n",
    "\n",
    "Calculate bag_of_words descriptor of the crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image crop \n",
    "crop_rgb = np.array(Image.open(\"cnos_analysis/crop_proposals/crop1.png\").convert(\"RGB\").resize((420,420))) # (124, 157, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify\n",
    "# Divide each template into patches size 14*14*3\n",
    "patch_size = 14\n",
    "crop_patches = patchify(crop_rgb, (14,14,3), step=patch_size)\n",
    "# Shape(30, 30, 1, 14, 14, 3) for each patch - Flaten this array \n",
    "crop_patches = crop_patches.reshape(-1,14,14,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 14, 14, 3)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out the templates by clampping at 0,1 \n",
    "crop_mask_patches = np.clip(np.sum(crop_patches, axis=-1), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 14, 14)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_mask_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Center of an image is height/2, width/2 and here is (7,7)\n",
    "valid_crop_patches = list()\n",
    "valid_crop_mask_patches = list()\n",
    "center_y, center_x = patch_size // 2, patch_size // 2\n",
    "\n",
    "for crop_patch, crop_mask_patch in zip(crop_patches, crop_mask_patches):\n",
    "    if crop_mask_patch[center_y, center_x] == 1:  # Check if the center belongs to the foreground\n",
    "        valid_crop_patches.append(crop_patch)\n",
    "        valid_crop_mask_patches.append(crop_mask_patch)\n",
    "len(valid_crop_patches) # see we have 410 valid patches for the crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([410, 1024])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_patches_features = patches_feature_extraction(np.array(valid_crop_patches), model)\n",
    "crop_patches_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410, 256)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=256)\n",
    "pca_crop_patches_descriptors = pca.fit_transform(np.array(crop_patches_features.cpu()))\n",
    "pca_crop_patches_descriptors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual words vector of 2048, 256 correponding to 2048 clusters/centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_clusters': 2048,\n",
       " 'init': 'k-means++',\n",
       " 'max_iter': 100,\n",
       " 'tol': 0.0,\n",
       " 'n_init': 'auto',\n",
       " 'verbose': 0,\n",
       " 'random_state': None,\n",
       " 'max_no_improvement': 10,\n",
       " 'batch_size': 1024,\n",
       " 'compute_labels': True,\n",
       " 'init_size': None,\n",
       " 'reassignment_ratio': 0.01,\n",
       " 'n_features_in_': 256,\n",
       " '_tol': 0,\n",
       " '_n_init': 1,\n",
       " '_batch_size': 1024,\n",
       " '_init_size': 3072,\n",
       " '_n_threads': 12,\n",
       " '_counts': array([ 22., 146.,  86., ...,  17.,  55.,  40.], dtype=float32),\n",
       " '_ewa_inertia': 53.83502694066241,\n",
       " '_ewa_inertia_min': 53.82570829866692,\n",
       " '_no_improvement': 10,\n",
       " '_n_since_last_reassign': 9216,\n",
       " 'cluster_centers_': array([[ 1.8226357 ,  9.511505  ,  0.8864465 , ..., -0.15399498,\n",
       "          0.10590126,  0.09780528],\n",
       "        [-5.6358614 , -0.38199693, -0.6433662 , ...,  0.15722364,\n",
       "         -0.15294628, -0.06539908],\n",
       "        [ 2.4246922 ,  2.4852493 ,  4.4770713 , ...,  0.01751995,\n",
       "         -0.13988431, -0.05914092],\n",
       "        ...,\n",
       "        [ 0.5268319 , -7.666996  ,  6.0609345 , ..., -0.2700177 ,\n",
       "          0.02096492, -0.16671124],\n",
       "        [ 9.96377   , -3.6719234 ,  0.25164425, ...,  0.02063943,\n",
       "          0.18797755,  0.08937422],\n",
       "        [-1.626545  , 10.126322  ,  1.215985  , ...,  0.06851327,\n",
       "          0.04164479,  0.08661783]], dtype=float32),\n",
       " '_n_features_out': 2048,\n",
       " 'n_steps_': 216,\n",
       " 'n_iter_': 7,\n",
       " 'labels_': array([ 901,   28,   23, ..., 1828, 1463, 1177], dtype=int32),\n",
       " 'inertia_': 1905603.0}"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 256)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
