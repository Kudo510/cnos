{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Synthetic to real domain gap\n",
    "Take some real image crops from  Sam， with less occlusions，as the templates. And mach them with the raw crops from Sam. You can maybe apply a threshold for the Bounding box size to filter the valid template. See if you can get some reasonable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Take and save some real image crops from SAM\n",
    "Take input image then put via sam then choose some of the crops- save in the cnos analysis folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sam\n",
    "from src.model.sam import CustomSamAutomaticMaskGenerator, load_sam\n",
    "from segment_anything.modeling.sam import Sam\n",
    "model_type = \"vit_h\"\n",
    "checkpoint_dir =  \"datasets/bop23_challenge/pretrained/segment-anything\"\n",
    "sam_model = load_sam(model_type, checkpoint_dir)\n",
    "custom_sam_model = CustomSamAutomaticMaskGenerator(sam=sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(segmentor_model, device=\"cuda:0\"):\n",
    "    # if there is predictor in the model, move it to device\n",
    "    if hasattr(segmentor_model, \"predictor\"):\n",
    "        segmentor_model.predictor.model = (\n",
    "            segmentor_model.predictor.model.to(device)\n",
    "        )\n",
    "    else:\n",
    "        segmentor_model.model.setup_model(device=device, verbose=True)\n",
    "move_to_device(custom_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.model.utils import Detections\n",
    "rgb_path = \"datasets/bop23_challenge/datasets/icbin/test/000001/rgb/000001.png\"\n",
    "rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "detections = custom_sam_model.generate_masks(np.array(rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_object_by_mask(image, mask, width: int = 512):\n",
    "    mask = Image.fromarray(mask)\n",
    "    masked_image = Image.composite(\n",
    "        image, Image.new(\"RGB\", image.size, (0, 0, 0)), mask)\n",
    "    cropped_image = masked_image.crop(masked_image.getbbox())\n",
    "    # new_height = width * cropped_image.height // cropped_image.width\n",
    "    return cropped_image\n",
    "\n",
    "masked_images = []\n",
    "for mask in detections[\"masks\"].cpu():\n",
    "    binary_mask = np.array(mask) * 255\n",
    "    binary_mask = binary_mask.astype(np.uint8)\n",
    "    masked_image = extract_object_by_mask(image, binary_mask)\n",
    "    masked_images.append(masked_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize proposals\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "def plot_images(images, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 30))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= len(images):\n",
    "            break\n",
    "        ax.imshow(images[i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define the number of rows and columns in the grid\n",
    "rows = ceil(len(masked_images) / 6)\n",
    "cols = 6\n",
    "# Plot the masked_images array in a grid\n",
    "# plot_images(masked_images, rows, cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are choosing proposals 2 and 8\n",
    "crop1 = {\n",
    "    \"crop\" : masked_images[1], # it is saved as Image so crop1[\"crop\"] to visualize it\n",
    "    \"masks\" : detections[\"masks\"][1],\n",
    "    \"boxes\" : detections[\"boxes\"][1]\n",
    "}\n",
    "crop2 = {\n",
    "    \"crop\" : masked_images[7],\n",
    "    \"masks\" : detections[\"masks\"][7],\n",
    "    \"boxes\" : detections[\"boxes\"][7]\n",
    "}\n",
    "\n",
    "# save image\n",
    "crop1[\"crop\"].save(\"cnos_analysis/crop_proposals/crop1.png\")\n",
    "crop2[\"crop\"].save(\"cnos_analysis/crop_proposals/crop2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create and save 42 templates from the test folder using BlenderProc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# Initialize Hydra and compose the configuration\n",
    "initialize(config_path=\"configs\")\n",
    "cfg = compose(config_name=\"run_inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10* set_struct to so that we can add key-value to the file - see lb we add new key-value root_dir to ref_dataloader_config\n",
    "OmegaConf.set_struct(cfg, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ref_dataloader_config = cfg.data.reference_dataloader\n",
    "ref_dataloader_config = default_ref_dataloader_config.copy()\n",
    "ref_dataloader_config._target_ = \"src.dataloader.bop_pbr.BOPTemplatePBR\"\n",
    "ref_dataloader_config.root_dir = \"datasets/bop23_challenge/datasets\"\n",
    "ref_dataloader_config.template_dir += \"templates_pyrender/icbin\" # we r working on icbin dataset\n",
    "ref_dataloader_config.templates_output_folder = \"cnos_analysis/real_images_templates\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_lite.utilities.seed:Global seed set to 2023\n",
      "INFO:lightning_lite.utilities.seed:Global seed set to 2023\n"
     ]
    },
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target 'src.dataloader.bop_pbr.BOPTemplatePBR':\nFileNotFoundError(2, 'No such file or directory')\nfull_key: data.reference_dataloader",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/cnos/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_target_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Werkstudent_job/ren_luyen/do_an_tot_nghiep/cnos/src/dataloader/bop_pbr.py:58\u001b[0m, in \u001b[0;36mBOPTemplatePBR.__init__\u001b[0;34m(self, root_dir, template_dir, processing_config, level_templates, pose_distribution, split, min_visib_fract, max_num_scenes, max_num_frames, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m split \u001b[38;5;66;03m# train_pbr\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_list_scene\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_scenes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m scene, but using only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_num_scenes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m scene for faster runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/Werkstudent_job/ren_luyen/do_an_tot_nghiep/cnos/src/dataloader/base_bop.py:42\u001b[0m, in \u001b[0;36mBaseBOP.load_list_scene\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m     38\u001b[0m         split_folder \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, split)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_scenes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m     40\u001b[0m         [\n\u001b[1;32m     41\u001b[0m             osp\u001b[38;5;241m.\u001b[39mjoin(split_folder, scene)\n\u001b[0;32m---> 42\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m scene \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(osp\u001b[38;5;241m.\u001b[39mjoin(split_folder, scene))\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m scene \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m         ]\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/bop23_challenge/datasets/train_pbr'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInstantiationException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhydra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m instantiate\n\u001b[0;32m----> 2\u001b[0m ref_dataset \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_dataloader_config\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# src.dataloader.bop.BOPTemplatePBR\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ref_dataset\u001b[38;5;241m.\u001b[39mload_processed_metaData(reset_metaData\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cnos/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     _convert_ \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(_Keys\u001b[38;5;241m.\u001b[39mCONVERT, ConvertMode\u001b[38;5;241m.\u001b[39mNONE)\n\u001b[1;32m    224\u001b[0m     _partial_ \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(_Keys\u001b[38;5;241m.\u001b[39mPARTIAL, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstantiate_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_recursive_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_partial_\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m OmegaConf\u001b[38;5;241m.\u001b[39mis_list(config):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# Finalize config (convert targets to strings, merge with kwargs)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     config_copy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/miniconda3/envs/cnos/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001b[0m, in \u001b[0;36minstantiate_node\u001b[0;34m(node, convert, recursive, partial, *args)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 value \u001b[38;5;241m=\u001b[39m instantiate_node(\n\u001b[1;32m    343\u001b[0m                     value, convert\u001b[38;5;241m=\u001b[39mconvert, recursive\u001b[38;5;241m=\u001b[39mrecursive\n\u001b[1;32m    344\u001b[0m                 )\n\u001b[1;32m    345\u001b[0m             kwargs[key] \u001b[38;5;241m=\u001b[39m _convert_node(value, convert)\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_target_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# If ALL or PARTIAL non structured or OBJECT non structured,\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# instantiate in dict and resolve interpolations eagerly.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert \u001b[38;5;241m==\u001b[39m ConvertMode\u001b[38;5;241m.\u001b[39mALL \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m         convert \u001b[38;5;129;01min\u001b[39;00m (ConvertMode\u001b[38;5;241m.\u001b[39mPARTIAL, ConvertMode\u001b[38;5;241m.\u001b[39mOBJECT)\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m node\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mobject_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    354\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/cnos/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_key:\n\u001b[1;32m     96\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfull_key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InstantiationException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mInstantiationException\u001b[0m: Error in call to target 'src.dataloader.bop_pbr.BOPTemplatePBR':\nFileNotFoundError(2, 'No such file or directory')\nfull_key: data.reference_dataloader"
     ]
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "ref_dataset = instantiate(ref_dataloader_config) # src.dataloader.bop.BOPTemplatePBR\n",
    "ref_dataset.load_processed_metaData(reset_metaData=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scale\n",
    "Try to zoom in and zoom out the templates or real image crops for the Sam instance， see if the similarly score changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Occlusion\n",
    "Find a real image crop without occlusion， and maskout some area of it，see how much the similarly score decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
