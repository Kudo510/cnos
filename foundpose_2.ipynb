{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Patch descriptors register in 3D for 642 templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Load templates\n",
    "template_path = \"datasets/bop23_challenge/datasets/templates_pyrender/icbin_642/obj_000001\"\n",
    "template_files = sorted(glob.glob(os.path.join(template_path, \"*.png\")), key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "templates = [np.array(Image.open(template_file).convert(\"RGB\").resize((420,420)))[:,:,:3] for template_file in template_files] # This image has 4 channels- the last one is not crucial - maybe about opacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def patches_feature_extraction(template_patches, dinov2_vitl14, device):\n",
    "    # crop_rgb: numpy array\n",
    "    # temps = np.transpose(np.array(template_patches), (0,2,3,1))\n",
    "    rgb_normalize = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "    normalized_patches = torch.stack([rgb_normalize(patch) for patch in template_patches])\n",
    "    layers_list = list(range(18))\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad(): \n",
    "        feature_patches= dinov2_vitl14.module.get_intermediate_layers(normalized_patches.to(device), n=layers_list, reshape=True, return_class_token=True)\n",
    "    return feature_patches[-1][0] # Choose the last features as the feature after the 18th layer, 1 fo class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "dinov2_vitl14.patch_size = 14\n",
    "if torch.cuda.is_available():\n",
    "    dinov2_vitl14 = torch.nn.DataParallel(dinov2_vitl14).to(device)  # Use DataParallel for multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "template_batches = [templates[i:i+batch_size] for i in range(0, len(templates), batch_size)]\n",
    "patch_features= list()\n",
    "\n",
    "for batch in template_batches:\n",
    "    torch.cuda.empty_cache()\n",
    "    batch_feature = patches_feature_extraction(batch, dinov2_vitl14, device)\n",
    "    patch_features.append(batch_feature.to('cpu'))\n",
    "    del batch_feature\n",
    "patch_features = torch.cat(patch_features).permute(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dinov2_vitl14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([642, 900, 1024])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_features = patch_features.reshape(642,-1,1024)\n",
    "patch_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filter out invalid templates\n",
    "Resize the mask to 30*30 and then choose the pixel position e.g (1,1) then check if mask[1,1] = 1 then the patch [1,1] is valid, otherwise it isn't "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize templates by 30,30\n",
    "resized_templates = [np.array(Image.open(template_file).convert(\"RGB\").resize((30,30)))[:,:,:3] for template_file in template_files] # This image has 4 channels- the last one is not crucial - maybe about opacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out the templates by clampping at 0,1 \n",
    "masks = np.clip(np.sum(resized_templates, axis=-1), 0, 1).reshape(642,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 900)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_valid_patches = list() # List of numbers of valid patches for each template\n",
    "valid_patch_features = list()\n",
    "for patch_feature, mask in zip(patch_features, masks):\n",
    "    valid_patches = patch_feature[mask==1]\n",
    "    valid_patch_features.append(valid_patches)\n",
    "    num_valid_patches.append(valid_patches.shape[0]) # Append number of  valid patches for the template to the list\n",
    "valid_patch_features = torch.cat(valid_patch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47568, 1024])\n",
      "642 [42, 52, 48, 48, 51, 51, 54, 55, 56, 53, 53, 59, 56, 55, 56, 58, 63, 58, 63, 59, 62, 59, 59, 60, 61, 60, 64, 62, 64, 64, 64, 67, 66, 68, 66, 63, 66, 68, 66, 67, 68, 66, 63, 64, 68, 68, 68, 67, 64, 66, 68, 72, 68, 70, 71, 71, 69, 67, 69, 70, 68, 71, 74, 73, 71, 71, 74, 69, 71, 72, 71, 74, 71, 71, 72, 73, 73, 75, 75, 75, 71, 73, 75, 73, 75, 75, 75, 73, 73, 75, 75, 76, 75, 75, 75, 71, 74, 74, 72, 77, 75, 78, 75, 73, 74, 76, 76, 78, 76, 77, 80, 77, 74, 73, 80, 77, 76, 78, 76, 74, 79, 78, 74, 74, 78, 77, 78, 77, 79, 78, 75, 74, 74, 75, 78, 79, 81, 79, 75, 75, 80, 79, 80, 80, 80, 76, 78, 80, 78, 79, 81, 80, 76, 76, 81, 80, 80, 79, 78, 78, 82, 81, 77, 77, 82, 81, 80, 79, 82, 80, 77, 79, 77, 77, 82, 82, 81, 80, 78, 77, 82, 78, 80, 79, 79, 82, 81, 82, 80, 78, 78, 79, 80, 79, 78, 82, 80, 77, 78, 81, 81, 81, 80, 81, 80, 78, 78, 76, 77, 81, 81, 81, 80, 80, 80, 76, 77, 76, 77, 80, 80, 76, 76, 78, 78, 74, 76, 77, 76, 74, 78, 78, 79, 78, 74, 73, 77, 76, 74, 73, 78, 77, 74, 73, 78, 78, 77, 77, 74, 72, 78, 77, 77, 77, 77, 74, 73, 74, 73, 78, 77, 76, 73, 75, 72, 76, 77, 77, 76, 71, 71, 76, 73, 74, 71, 76, 77, 77, 76, 70, 72, 75, 75, 75, 75, 72, 71, 71, 71, 76, 76, 76, 75, 73, 71, 73, 76, 70, 72, 76, 76, 76, 76, 76, 76, 77, 76, 75, 75, 75, 75, 76, 77, 75, 75, 75, 75, 75, 77, 76, 76, 76, 76, 76, 76, 75, 74, 73, 73, 73, 74, 74, 74, 74, 73, 73, 73, 73, 75, 75, 76, 73, 74, 76, 78, 77, 76, 78, 76, 76, 75, 74, 74, 76, 78, 78, 77, 77, 76, 77, 75, 77, 78, 76, 75, 73, 74, 78, 78, 76, 76, 77, 78, 75, 75, 74, 75, 77, 78, 77, 78, 74, 75, 77, 79, 78, 78, 77, 77, 76, 75, 74, 77, 78, 77, 75, 74, 75, 77, 78, 76, 75, 77, 78, 78, 77, 77, 78, 76, 76, 74, 75, 77, 78, 76, 78, 77, 75, 75, 76, 79, 74, 75, 74, 77, 76, 78, 76, 77, 76, 75, 74, 75, 74, 77, 77, 76, 78, 77, 75, 78, 75, 76, 74, 75, 76, 75, 77, 78, 78, 77, 74, 75, 78, 75, 76, 76, 78, 77, 78, 78, 76, 77, 77, 79, 77, 77, 78, 77, 75, 78, 77, 80, 76, 80, 78, 78, 78, 77, 75, 81, 78, 77, 76, 78, 81, 78, 78, 78, 78, 81, 78, 76, 78, 80, 76, 78, 78, 78, 76, 78, 78, 76, 78, 80, 76, 77, 76, 75, 78, 76, 78, 79, 76, 77, 80, 76, 76, 75, 76, 78, 76, 76, 76, 78, 77, 76, 76, 75, 75, 76, 76, 77, 77, 77, 78, 75, 76, 76, 76, 75, 76, 77, 77, 76, 77, 78, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 76, 76, 76, 76, 77, 77, 77, 76, 76, 76, 74, 75, 76, 74, 73, 73, 74, 73, 73, 73, 75, 73, 74, 72, 71, 71, 71, 71, 73, 71, 71, 72, 72, 72, 70, 69, 71, 72, 70, 70, 68, 66, 67, 69, 68, 66, 68, 68, 68, 66, 69, 68, 66, 67, 65, 64, 66, 66, 65, 65, 63, 67, 66, 66, 64, 64, 67, 64, 66, 65, 65, 64, 64, 64, 63, 63, 64, 63, 63, 65, 64, 64, 65, 65, 65]\n"
     ]
    }
   ],
   "source": [
    "print(valid_patch_features.shape)\n",
    "print(len(num_valid_patches), num_valid_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47568, 256)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with the 256 components to reduce dimensionality of the features\n",
    "pca = PCA(n_components=256)\n",
    "pca_patches_descriptors = pca.fit_transform(np.array(valid_patch_features.cpu()))\n",
    "pca_patches_descriptors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perform Kmean clustering for all patch descriptors from templates (2048 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 47568 points to 2048 centroids: please provide at least 79872 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 47568 points in 256D to 2048 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 19 (0.27 s, search 0.15 s): objective=2.22272e+07 imbalance=1.167 nsplit=0       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22227172.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/facebookresearch/faiss/wiki/Faiss-building-blocks:-clustering,-PCA,-quantization\n",
    "import faiss\n",
    "ncentroids = 2048\n",
    "niter = 20\n",
    "verbose = True\n",
    "d = pca_patches_descriptors.shape[1]\n",
    "kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose, gpu=True)\n",
    "kmeans.train(pca_patches_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47568, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign labels to the data points\n",
    "labels = kmeans.index.search(pca_patches_descriptors, 1)[1]\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_labels = list()\n",
    "start_idx = 0\n",
    "for num in num_valid_patches:\n",
    "    end_idx = start_idx + num\n",
    "    template_labels = labels[start_idx:end_idx].reshape(-1)\n",
    "    templates_labels.append(template_labels)\n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_templates_vector(templates_labels, num_clusters = 2048):\n",
    "    # Calculate bag-of-words descriptors of the templates\n",
    "\n",
    "    templates_vector = list()\n",
    "    all_occurrences = [np.bincount(templates_label, minlength=2048) for templates_label in templates_labels]\n",
    "    ni_array = np.sum(np.array(all_occurrences), axis = 0)\n",
    "    N = len(templates_labels) # Number of templates\n",
    "    for t in range(len(templates_labels)):\n",
    "        template_vector = list()\n",
    "        occurrences = np.bincount(templates_labels[t], minlength=2048)\n",
    "        for i in range(num_clusters):\n",
    "            n_it = occurrences[i]\n",
    "            nt = len(templates_labels[t])\n",
    "            ni = ni_array[i]\n",
    "            bi = n_it / nt * math.log(N / ni)\n",
    "            template_vector.append(bi)\n",
    "        templates_vector.append(np.array(template_vector))\n",
    "    return templates_vector\n",
    "templates_vector = calculate_templates_vector(templates_labels = templates_labels, num_clusters = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4176330249533452"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(templates_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieving similar templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image crop \n",
    "crop_rgb = np.array(Image.open(\"cnos_analysis/crop_proposals/crop1.png\").convert(\"RGB\").resize((420,420))) # (124, 157, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 420, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cuong.vandam/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "dinov2_vitl14.patch_size = 14\n",
    "if torch.cuda.is_available():\n",
    "    dinov2_vitl14 = torch.nn.DataParallel(dinov2_vitl14).to(device)  # Use DataParallel for multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_feature = patches_feature_extraction(np.expand_dims(crop_rgb, 0), dinov2_vitl14, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 30, 30])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dinov2_vitl14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valid_crop_features \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_feature\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m30\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:\u001b[38;5;241m400\u001b[39m,:]\n\u001b[1;32m      2\u001b[0m valid_crop_features\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "valid_crop_features = crop_feature[0].permute(0,2,3,1).reshape(30*30,-1)[:400,:]\n",
    "valid_crop_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 256)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=256)\n",
    "pca_crop_patches_descriptors = pca.fit_transform(np.array(valid_crop_features.cpu()))\n",
    "pca_crop_patches_descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign labels to the data points\n",
    "crop_labels = kmeans.index.search(pca_crop_patches_descriptors, 1)[1].reshape(-1)\n",
    "crop_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350848/2790732940.py:17: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  bi = n_it / nt * math.log(N / ni)\n",
      "/tmp/ipykernel_350848/2790732940.py:17: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  bi = n_it / nt * math.log(N / ni)\n"
     ]
    }
   ],
   "source": [
    "crop_vector = calculate_templates_vector(templates_labels = [crop_labels], num_clusters = 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# For word_i, term frequency = occurences of word_i within the crop / number of occurences of word_i in all templates). \n",
    "def calculate_crop_vector(crop_labels, templates_labels, num_clusters = 2048):\n",
    "    # Calculate bag-of-words descriptors of the templates\n",
    "    all_occurrences_crop = np.bincount(crop_labels, minlength=2048)\n",
    "\n",
    "    all_occurrences_templates = [np.bincount(templates_label, minlength=2048) for templates_label in templates_labels]\n",
    "    ni_array = np.sum(np.array(all_occurrences_templates), axis = 0)\n",
    "    N = len(templates_labels) # Number of templates = 642 \n",
    "\n",
    "    crop_vector = list()\n",
    "    for i in range(num_clusters):\n",
    "        n_it = all_occurrences_crop[i]\n",
    "        nt = crop_labels.shape[0] # Number of words in crop = 400 \n",
    "        ni = ni_array[i]\n",
    "        bi = n_it / nt * math.log(N / ni)\n",
    "        crop_vector.append(bi)\n",
    "    return torch.tensor(crop_vector).view(1,-1) # Goal having features size (1,2048)\n",
    "crop_vector = calculate_crop_vector(crop_labels = crop_labels, templates_labels = templates_labels, num_clusters = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_templates_vector = torch.cat([torch.tensor(vector).view(1,-1) for vector in templates_vector]) # Goal torch.Size([642, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([642, 2048])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_templates_vector.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieve top similar pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.model.loss import PairwiseSimilarity, Similarity\n",
    "from torchvision.io import read_image\n",
    "metric = Similarity() # cnos  uses PairwiseSimilariy - what is the difference between Similarity and Pairwise Similarity ?\n",
    "\n",
    "def calculate_similarity(crop_rgb, feature_decriptors, ref_features, metric, synthetic=False):\n",
    "    # get scores per proposal\n",
    "    scores = metric(feature_decriptors[:, None, :], ref_features[None, :, :]) # should get  # N_proposals x N_objects x N_templates -get only 1,42 as num_prosals*num_templates instead\n",
    "    score_per_detection, similar_template_indices = torch.topk(scores, k=5, dim=-1) # get top 5 most similar templates\n",
    "    # get the final confidence score\n",
    "    score_per_detection = torch.mean(\n",
    "        score_per_detection, dim=-1\n",
    "    ) \n",
    "    # Check the confidence scores for the similar templates\n",
    "    similar_scores = scores[:, similar_template_indices[0].to(\"cpu\")]\n",
    "\n",
    "    similar_templates = []\n",
    "    for i in range(len(similar_template_indices[0])):\n",
    "        if synthetic:\n",
    "            img = read_image(f\"cnos_analysis/real_images_templates/icbin/train_pbr_templates/obj_000001_original/{(similar_template_indices[0][i]):06d}.png\")            \n",
    "        else:\n",
    "            img = read_image(f\"cnos_analysis/real_images_templates/icbin/obj_000001_original/{(similar_template_indices[0][i]):06d}.png\")        \n",
    "        similar_templates.append(img)\n",
    "    tempplate_images = torch.stack(similar_templates)\n",
    "\n",
    "    # Display the crop\n",
    "    plt.imshow(crop_rgb)\n",
    "    plt.axis('off')  # Optional: Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "    print(\"top 5 confidence scores\", similar_scores)\n",
    "    print(\"final average confidence score\", score_per_detection)\n",
    "\n",
    "    w = 10\n",
    "    h = 10\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "    for i in range(1, len(tempplate_images)+1):\n",
    "        img = np.random.randint(10, size=(h,w))\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(tempplate_images[i-1].permute(1,2,0))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
