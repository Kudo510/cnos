{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo EfficientLoFTR on a single pair of images\n",
    "\n",
    "This notebook shows how to use the eloftr matcher with different model type and numerical precision on the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outdoor Example\n",
    "\n",
    "We recommend using our pre-trained model for input in outdoor environments because our model has only been trained on MegaDepth, and there exists a domain gap between indoor and outdoor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backbone_type': 'RepVGG', 'align_corner': False, 'resolution': (8, 1), 'fine_window_size': 8, 'mp': False, 'replace_nan': True, 'half': True, 'backbone': {'block_dims': [64, 128, 256]}, 'coarse': {'d_model': 256, 'd_ffn': 256, 'nhead': 8, 'layer_names': ['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross'], 'agg_size0': 4, 'agg_size1': 4, 'no_flash': False, 'rope': True, 'npe': [832, 832, 832, 832]}, 'match_coarse': {'thr': 0.2, 'border_rm': 2, 'dsmax_temperature': 0.1, 'skip_softmax': False, 'fp16matmul': False, 'train_coarse_percent': 0.2, 'train_pad_num_gt_min': 200}, 'match_fine': {'local_regress_temperature': 10.0, 'local_regress_slicedim': 8}}\n"
     ]
    }
   ],
   "source": [
    "# Load example images\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from src_eloftr.utils.plotting import make_matching_figure\n",
    "from src_eloftr.loftr import LoFTR, full_default_cfg, opt_default_cfg, reparameter\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# You can choose model type in ['full', 'opt']\n",
    "model_type = 'full' # 'full' for best quality, 'opt' for best efficiency\n",
    "\n",
    "# You can choose numerical precision in ['fp32', 'mp', 'fp16']. 'fp16' for best efficiency\n",
    "precision = 'fp16' # Enjoy near-lossless precision with Mixed Precision (MP) / FP16 computation if you have a modern GPU (recommended NVIDIA architecture >= SM_70).\n",
    "\n",
    "# You can also change the default values like thr. and npe (based on input image size)\n",
    "\n",
    "if model_type == 'full':\n",
    "    _default_cfg = deepcopy(full_default_cfg)\n",
    "elif model_type == 'opt':\n",
    "    _default_cfg = deepcopy(opt_default_cfg)\n",
    "    \n",
    "if precision == 'mp':\n",
    "    _default_cfg['mp'] = True\n",
    "elif precision == 'fp16':\n",
    "    _default_cfg['half'] = True\n",
    "    \n",
    "print(_default_cfg)\n",
    "matcher = LoFTR(config=_default_cfg)\n",
    "\n",
    "matcher.load_state_dict(torch.load(\"src_eloftr/weights/eloftr_outdoor.ckpt\")['state_dict'])\n",
    "matcher = reparameter(matcher) # no reparameterization will lead to low performance\n",
    "\n",
    "if precision == 'fp16':\n",
    "    matcher = matcher.half()\n",
    "\n",
    "matcher = matcher.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_correspondences_original(img0_pth, img1_pth, matcher=matcher):\n",
    "\n",
    "    img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n",
    "    img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//32*32, img0_raw.shape[0]//32*32))  # input size shuold be divisible by 32\n",
    "    # img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//32*32, img1_raw.shape[0]//32*32))\n",
    "    img0_raw = cv2.resize(img0_raw, (224, 224))  # input size shuold be divisible by 32\n",
    "    img1_raw = cv2.resize(img1_raw, (224, 224))\n",
    "\n",
    "    if precision == 'fp16':\n",
    "        img0 = torch.from_numpy(img0_raw)[None][None].half().cuda() / 255.\n",
    "        img1 = torch.from_numpy(img1_raw)[None][None].half().cuda() / 255.\n",
    "        # img1 = torch.concat((img1, img1), 0)\n",
    "        # img0 = torch.from_numpy(img0_raw)[None].half().permute(0,3,1,2).cuda() / 255. # self.half() is equivalent to self.to(torch.float16)\n",
    "        # img1 = torch.from_numpy(img1_raw)[None].half().permute(0,3,1,2).cuda() / 255.\n",
    "\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with EfficientLoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "        if precision == 'mp':\n",
    "            with torch.autocast(enabled=True, device_type='cuda'):\n",
    "                matcher(batch)\n",
    "        else:\n",
    "            matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "    \n",
    "    # if model_type == 'opt':\n",
    "    #     print(mconf.max())\n",
    "    #     mconf = (mconf - min(20.0, mconf.min())) / (max(30.0, mconf.max()) - min(20.0, mconf.min()))\n",
    "\n",
    "    # color = cm.jet(mconf)\n",
    "    # text = [\n",
    "    #     'LoFTR',\n",
    "    #     'Matches: {}'.format(len(mkpts0)),\n",
    "    # ]\n",
    "\n",
    "    # fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)\n",
    "\n",
    "    return len(mkpts0) # number of corresponeces between 2 images\n",
    "\n",
    "\n",
    "def extract_correspondences(img0_pth, img1_pth, matcher=matcher):\n",
    "    '''\n",
    "    input templates as rgb images ( not transform) and sam proposals as rgb image\n",
    "    output: max number of corres outof 42 templates- jsut 42 templates to save time\n",
    "    '''\n",
    "\n",
    "    img0_raw = torch.tensor(np.array(Image.open(img0_pth))/255.0).permute(2,0,1).cuda()\n",
    "    img1_raw = torch.tensor(np.array(Image.open(img1_pth))/255.0).permute(2,0,1).cuda()\n",
    "\n",
    "    img0_raw = 0.2989 * img0_raw[0] + 0.5870 * img0_raw[1] + 0.114 * img0_raw[2]\n",
    "    img1_raw = 0.2989 * img1_raw[0] + 0.5870 * img1_raw[1] + 0.114 * img1_raw[2]\n",
    "\n",
    "    # img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n",
    "    # img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//32*32, img0_raw.shape[0]//32*32))  # input size shuold be divisible by 32\n",
    "    # img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//32*32, img1_raw.shape[0]//32*32))\n",
    "\n",
    "    if precision == 'fp16':\n",
    "        img0 = img0_raw[None][None].half() # self.half() is equivalent to self.to(torch.float16)\n",
    "        # img0 = torch.concat((img0, img0), 0)\n",
    "        img1 = img1_raw[None][None].half()\n",
    "        # img0 = torch.concatenate((img0, img0), dim = 0)\n",
    "        # img1 = torch.concatenate((img1, img1), dim = 0)\n",
    "        # img1 = torch.concat((img1, img1), 0)\n",
    "        # img0 = torch.from_numpy(img0_raw)[None].half().permute(0,3,1,2).cuda() / 255. # self.half() is equivalent to self.to(torch.float16)\n",
    "        # img1 = torch.from_numpy(img1_raw)[None].half().permute(0,3,1,2).cuda() / 255.\n",
    "\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with EfficientLoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "        if precision == 'mp':\n",
    "            with torch.autocast(enabled=True, device_type='cuda'):\n",
    "                matcher(batch)\n",
    "        else:\n",
    "            matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "    \n",
    "    # if model_type == 'opt':\n",
    "    #     print(mconf.max())\n",
    "    #     mconf = (mconf - min(20.0, mconf.min())) / (max(30.0, mconf.max()) - min(20.0, mconf.min()))\n",
    "\n",
    "    # color = cm.jet(mconf)\n",
    "    # text = [\n",
    "    #     'LoFTR',\n",
    "    #     'Matches: {}'.format(len(mkpts0)),\n",
    "    # ]\n",
    "\n",
    "    # fig = make_matching_figure(np.array(Image.open(img0_pth)), np.array(Image.open(img1_pth)), mkpts0, mkpts1, color, text=text)\n",
    "\n",
    "    return len(mkpts0) # number of corresponeces between 2 images\n",
    "\n",
    "\n",
    "def extract_correspondences_batches(img0_pth_list, img1_pth_list, matcher=matcher):\n",
    "    '''\n",
    "    input templates as rgb images ( not transform) and sam proposals as rgb image\n",
    "    output: max number of corres outof 42 templates- jsut 42 templates to save time\n",
    "    '''\n",
    "\n",
    "    img0_list = list()\n",
    "    img1_list = list()\n",
    "    for img0_pth in img0_pth_list:\n",
    "        img0_raw = torch.tensor(np.array(Image.open(img0_pth))/255.0).permute(2,0,1).cuda()\n",
    "        img0_raw = 0.2989 * img0_raw[0] + 0.5870 * img0_raw[1] + 0.114 * img0_raw[2]\n",
    "        img0_list.append(img0_raw[None].half())\n",
    "\n",
    "    for img1_pth in img1_pth_list:\n",
    "        img1_raw = torch.tensor(np.array(Image.open(img1_pth))/255.0).permute(2,0,1).cuda()\n",
    "        img1_raw = 0.2989 * img1_raw[0] + 0.5870 * img1_raw[1] + 0.114 * img1_raw[2]\n",
    "        img1_list.append(img1_raw[None].half())\n",
    "\n",
    "    # img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n",
    "    # img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//32*32, img0_raw.shape[0]//32*32))  # input size shuold be divisible by 32\n",
    "    # img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//32*32, img1_raw.shape[0]//32*32))\n",
    "\n",
    "\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with EfficientLoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "        if precision == 'mp':\n",
    "            with torch.autocast(enabled=True, device_type='cuda'):\n",
    "                matcher(batch)\n",
    "        else:\n",
    "            matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "    \n",
    "    # if model_type == 'opt':\n",
    "    #     print(mconf.max())\n",
    "    #     mconf = (mconf - min(20.0, mconf.min())) / (max(30.0, mconf.max()) - min(20.0, mconf.min()))\n",
    "\n",
    "    # color = cm.jet(mconf)\n",
    "    # text = [\n",
    "    #     'LoFTR',\n",
    "    #     'Matches: {}'.format(len(mkpts0)),\n",
    "    # ]\n",
    "\n",
    "    # fig = make_matching_figure(np.array(Image.open(img0_pth)), np.array(Image.open(img1_pth)), mkpts0, mkpts1, color, text=text)\n",
    "\n",
    "    return len(mkpts0) # number of corresponeces between 2 images\n",
    "\n",
    "\n",
    "def extract_correspondences_original_batches(img0_pth_list, img1_pth_list, matcher=matcher):\n",
    "\n",
    "    img0_list = list()\n",
    "    img1_list = list()\n",
    "    for img0_pth in img0_pth_list:\n",
    "        img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n",
    "        # img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//32*32, img0_raw.shape[0]//32*32))  # input size shuold be divisible by 32\n",
    "        img0_raw = cv2.resize(img0_raw,(224,224))\n",
    "        img0_raw = torch.from_numpy(img0_raw)[None].half().cuda() / 255.\n",
    "        img0_list.append(img0_raw)\n",
    "\n",
    "    for img1_pth in img1_pth_list:\n",
    "        img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n",
    "        # img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//32*32, img1_raw.shape[0]//32*32))\n",
    "        img1_raw = cv2.resize(img1_raw, (224,224))\n",
    "        img1_raw = torch.from_numpy(img1_raw)[None].half().cuda() / 255.\n",
    "        img1_list.append(img1_raw)    \n",
    "\n",
    "    img0 = torch.stack(img0_list)\n",
    "    img1 = torch.stack(img1_list)\n",
    "\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with EfficientLoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "        if precision == 'mp':\n",
    "            with torch.autocast(enabled=True, device_type='cuda'):\n",
    "                matcher(batch)\n",
    "        else:\n",
    "            matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "    \n",
    "    # if model_type == 'opt':\n",
    "    #     print(mconf.max())\n",
    "    #     mconf = (mconf - min(20.0, mconf.min())) / (max(30.0, mconf.max()) - min(20.0, mconf.min()))\n",
    "\n",
    "    # color = cm.jet(mconf)\n",
    "    # text = [\n",
    "    #     'LoFTR',\n",
    "    #     'Matches: {}'.format(len(mkpts0)),\n",
    "    # ]\n",
    "\n",
    "    # fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)\n",
    "\n",
    "    return len(mkpts0) # number of corresponeces between 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "# Load example images\n",
    "\n",
    "# crops = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/crops/*.png\"))\n",
    "crops = [\"foundpose_analysis/daoliuzhao/crops/crop1.png\", \"foundpose_analysis/daoliuzhao/crops/crop2.png\", \"foundpose_analysis/daoliuzhao/crops/crop3.png\"]\n",
    "templates = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/templates/train_pbr/obj_000001_original/*.png\"))\n",
    "\n",
    "used_templates = templates[:3]\n",
    "num_correspondences = extract_correspondences_original_batches(crops, used_templates)\n",
    "num_correspondences\n",
    "# print(f\"max num correpodences: {max(num_corres_temp_list)}\")\n",
    "# print(f\"top 5 max num correpodences: {num_corres_temp_list[np.argsort(num_corres_temp_list)][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop name:  crop1.png\n",
      "Num correpodences: 10\n",
      "crop name:  crop2.png\n",
      "Num correpodences: 38\n",
      "crop name:  crop3.png\n",
      "Num correpodences: 12\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "# Load example images\n",
    "\n",
    "# crops = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/crops/*.png\"))\n",
    "# crops = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/crops/*.png\"))\n",
    "crops = [\"foundpose_analysis/daoliuzhao/crops/crop1.png\", \"foundpose_analysis/daoliuzhao/crops/crop2.png\", \"foundpose_analysis/daoliuzhao/crops/crop3.png\"]\n",
    "templates = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/templates/train_pbr/obj_000001_original/*.png\"))\n",
    "\n",
    "used_templates = templates[:3]\n",
    "num_corres_list = list()\n",
    "\n",
    "for i in range(len(crops)):\n",
    "    print(\"crop name: \", crops[i].split(\"/\")[-1])\n",
    "    num_corres_temp = extract_correspondences_original(crops[i], used_templates[i], matcher=matcher)\n",
    "    print(f\"Num correpodences: {num_corres_temp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop name:  xoa_3.png\n",
      "max num correpodences: 82\n",
      "top 5 max num correpodences: [50 54 56 64 82]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "# Load example images\n",
    "\n",
    "# crops = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/crops/*.png\"))\n",
    "crops = [\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/xoa_3.png\"]\n",
    "templates = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/templates/train_pbr/obj_000001_original/*.png\"))\n",
    "\n",
    "num_corres_list = list()\n",
    "\n",
    "for crop in crops[0:1]:\n",
    "    print(\"crop name: \", crop.split(\"/\")[-1])\n",
    "    num_corres_temp_list = np.array([extract_correspondences(crop, temp, matcher=matcher) for temp in templates])\n",
    "    print(f\"max num correpodences: {max(num_corres_temp_list)}\")\n",
    "    print(f\"top 5 max num correpodences: {num_corres_temp_list[np.argsort(num_corres_temp_list)][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 7, 9, 4, 10, 11]\n",
      "[7, 6, 4, 3, 2, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "original_list = [1, 5, 7, 9, 4, 10, 11]\n",
    "\n",
    "# Get the ranks of the elements in the original list\n",
    "ranks = sorted(range(len(original_list)), key=lambda i: -original_list[i]) \n",
    "ranks = [rank + 1 for rank in ranks]\n",
    "\n",
    "print(original_list)\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalso check eloftetr if it has better features- then can test it with cnos to see if we have better matching\\nother wirse just based the numer of corres to rank again- or to put weight on the scores\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "also check eloftetr if it has better features- then can test it with cnos to see if we have better matching\n",
    "other wirse just based the numer of corres to rank again- or to put weight on the scores\n",
    "'''\n",
    "\n",
    "# to do now is to load input the same way as in cnos- input as templates of rgb and sam proposals based on the detections not the mask_images- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop name:  xoa_3.png\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m crops:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop name: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crop\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m     num_corres_temp_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([extract_correspondences(crop, temp, matcher\u001b[38;5;241m=\u001b[39mmatcher) \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m templates])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(num_corres_temp_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop 5 max num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_corres_temp_list[np\u001b[38;5;241m.\u001b[39margsort(num_corres_temp_list)][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m crops:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop name: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crop\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m     num_corres_temp_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mextract_correspondences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m templates])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(num_corres_temp_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop 5 max num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_corres_temp_list[np\u001b[38;5;241m.\u001b[39margsort(num_corres_temp_list)][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mextract_correspondences\u001b[0;34m(img0_pth, img1_pth, matcher)\u001b[0m\n\u001b[1;32m     19\u001b[0m img0_raw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(img0_pth))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     20\u001b[0m img1_raw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(img1_pth))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 22\u001b[0m img0_raw \u001b[38;5;241m=\u001b[39m \u001b[43mrgb_to_grayscale_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg0_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m img1_raw \u001b[38;5;241m=\u001b[39m rgb_to_grayscale_pytorch(img1_raw)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//32*32, img0_raw.shape[0]//32*32))  # input size shuold be divisible by 32\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//32*32, img1_raw.shape[0]//32*32))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mrgb_to_grayscale_pytorch\u001b[0;34m(rgb_tensor)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Ensure the tensor is in shape (batch_size, channels, height, width) or (channels, height, width)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rgb_tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# Single image\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     grayscale_tensor \u001b[38;5;241m=\u001b[39m (\u001b[43mrgb_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rgb_tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:  \u001b[38;5;66;03m# Batch of images\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     grayscale_tensor \u001b[38;5;241m=\u001b[39m (rgb_tensor \u001b[38;5;241m*\u001b[39m weights[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m])\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop name:  crop0.png\n",
      "max num correpodences: 13\n",
      "top 5 max num correpodences: [11 11 12 12 13]\n",
      "crop name:  crop1.png\n",
      "max num correpodences: 18\n",
      "top 5 max num correpodences: [13 13 14 17 18]\n",
      "crop name:  crop10.png\n",
      "max num correpodences: 7\n",
      "top 5 max num correpodences: [5 6 6 6 7]\n",
      "crop name:  crop11.png\n",
      "max num correpodences: 12\n",
      "top 5 max num correpodences: [10 11 11 11 12]\n",
      "crop name:  crop12.png\n",
      "max num correpodences: 7\n",
      "top 5 max num correpodences: [6 6 6 6 7]\n",
      "crop name:  crop13.png\n",
      "max num correpodences: 8\n",
      "top 5 max num correpodences: [6 6 7 7 8]\n",
      "crop name:  crop14.png\n",
      "max num correpodences: 29\n",
      "top 5 max num correpodences: [23 24 24 25 29]\n",
      "crop name:  crop15.png\n",
      "max num correpodences: 0\n",
      "top 5 max num correpodences: [0 0 0 0 0]\n",
      "crop name:  crop16.png\n",
      "max num correpodences: 14\n",
      "top 5 max num correpodences: [11 12 12 13 14]\n",
      "crop name:  crop17.png\n",
      "max num correpodences: 8\n",
      "top 5 max num correpodences: [6 7 7 7 8]\n",
      "crop name:  crop18.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m crops:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop name: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crop\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m     num_corres_temp_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([extract_correspondences(crop, temp, matcher\u001b[38;5;241m=\u001b[39mmatcher) \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m templates])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(num_corres_temp_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop 5 max num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_corres_temp_list[np\u001b[38;5;241m.\u001b[39margsort(num_corres_temp_list)][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m crops:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop name: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crop\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m     num_corres_temp_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mextract_correspondences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m templates])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(num_corres_temp_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop 5 max num correpodences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_corres_temp_list[np\u001b[38;5;241m.\u001b[39margsort(num_corres_temp_list)][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mextract_correspondences\u001b[0;34m(img0_pth, img1_pth, matcher)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     img0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img0_raw)[\u001b[38;5;28;01mNone\u001b[39;00m][\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m \u001b[38;5;66;03m# self.half() is equivalent to self.to(torch.float16)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1_raw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# img0 = torch.from_numpy(img0_raw)[None].half().permute(0,3,1,2).cuda() / 255. # self.half() is equivalent to self.to(torch.float16)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# img1 = torch.from_numpy(img1_raw)[None].half().permute(0,3,1,2).cuda() / 255.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     img0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img0_raw)[\u001b[38;5;28;01mNone\u001b[39;00m][\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "# Load example images\n",
    "\n",
    "crops = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/foundpose_analysis/daoliuzhao/crops/*.png\"))\n",
    "templates = sorted(glob.glob(\"/home/cuong.van-dam/CuongVanDam/do_an_tot_nghiep/cnos/datasets/bop23_challenge/datasets/templates_pyrender/daoliuzhao/obj_000001/*.png\"))\n",
    "\n",
    "num_corres_list = list()\n",
    "\n",
    "for crop in crops:\n",
    "    print(\"crop name: \", crop.split(\"/\")[-1])\n",
    "    num_corres_temp_list = np.array([extract_correspondences(crop, temp, matcher=matcher) for temp in templates])\n",
    "    print(f\"max num correpodences: {max(num_corres_temp_list)}\")\n",
    "    print(f\"top 5 max num correpodences: {num_corres_temp_list[np.argsort(num_corres_temp_list)][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b8911f875a754a9ad2a8804064d078bf6a1985972bb0389b9d67771213c8e20"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('svcnn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
